<!DOCTYPE html>
<html>
<head>
    <title>GPT4SGG: Synthesizing Scene Graphs from Holistic and Region-specific Narratives</title>
    <link rel="stylesheet" type="text/css" href="style.css">
</head>
<body>
    <header>
        <div class="header-flex-container">
            <img src="resources/polyu-logo.png" alt="PolyU Logo" class="header-logo">
            <h1 class="header-title">GPT4SGG: Synthesizing Scene Graphs from Holistic and Region-specific Narratives</h1>
        </div>
    </header>
<nav>
<div class="author-affiliations">
  <span><a href="https://scholar.google.com/citations?user=kuQ_mrAAAAAJ&hl=en">Zuyao Chen</a><sup>1,2</sup></span>,
  <span><a href="https://scholar.google.com/citations?user=XujjZmUAAAAJ&hl=en">Jinlin Wu</a><sup>2,3</sup></span>,
  <span><a href="https://scholar.google.com/citations?user=cuJ3QG8AAAAJ&hl=en">Zhen Lei</a><sup>2,3</sup></span>,
  <span><a href="">Zhaoxiang Zhang</a><sup>2,3</sup></span>,
  <span><a href="https://scholar.google.com/citations?user=w2HXPUUAAAAJ&hl=en">Changwen Chen</a><sup>1</sup></span>
  <div class="affiliation">
    <sup>1</sup> The Hong Kong Polytechnic University
  </div>
    <a href="https://chenlab.comp.polyu.edu.hk" style="margin:50px;color:#003366;"> https://chenlab.comp.polyu.edu.hk </a>
  <div class="affiliation">
    <sup>2</sup> Centre for Artificial Intelligence and Robotics, HKISI, CAS
  </div>
  <div class="affiliation">
    <sup>3</sup> NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China
  </div>
</div>
<div class="buttons">
  <a href="https://arxiv.org/pdf/2312.04314.pdf" class="button">arXiv</a>
  <a href="" class="button">Code</a>
  <a href="" class="button">Demo</a>
  <a href="" class="button">Dataset</a>
  <a href="https://huggingface.co/JosephZ/gpt4sgg-llama2-13b-int8" class="button">Model</a>
</div>
</nav>
    <section>
        <div class="flex-container">
        <img src="resources/GPT4SGG-intro.png" style="width:70%;height:auto;" alt="Challenges in learning scene graphs from natural language description.">
	</div>
        <h2>Abstract</h2>
        <p> Training Scene Graph Generation (SGG) models with natural language captions has become increasingly popular due to the abundant, cost-effective, 
            and open-world generalization supervision signals that natural language offers.
            However, such unstructured caption data and its processing pose significant challenges in learning accurate and comprehensive scene graphs.
            The challenges can be summarized as three aspects: 
            <ul><li><b>1)</b> traditional scene graph parsers based on linguistic representation often fail to extract meaningful relationship triplets from caption data.
          </li><li><b>2)</b>  grounding unlocalized objects of parsed triplets will meet ambiguity issues in visual-language alignment. 
          </li><li><b>3)</b> caption data typically are sparse and exhibit bias to partial observations of image content.
          </ul> Aiming to address these problems,  we propose a divide-and-conquer strategy with a novel framework
          named  <span class="italic-text" style="color: #000080;"><b>GPT4SGG</b></span>, to obtain more accurate and comprehensive scene graph signals. 
          This framework decomposes a complex scene into a bunch of simple regions, 
          resulting in a set of region-specific narratives. With these region-specific narratives (partial observations) and a holistic narrative (global observation) for an image, 
          a large language model (LLM) performs the relationship reasoning to synthesize 
          an accurate and comprehensive scene graph.
    </section>
    <section>
	<h2> Method </h2>
        <div class="flex-container">
           <img src="resources/GPT4SGG-main.png" style="width:70%; height:auto;" alt="Overview of <b>GPT4SGG</b>.">
        </div>	
        <p><b>Textual representation of image data</b>: localised objects, holistic & region-specific narratives. </p>
        <p><b>Task-specific (SGG-aware) Prompt</b>: synthesize scene graphs based on the textual input for image data. </p> 	  
    </section>

    <section>
        <h2> Comparison with state-of-the-arts on VG150 test set, diamond symbol marks fully supervised methods </h2>
        <div class="flex-container">
           <img src="resources/perf.png" style="width:70%;height:auto;" alt="comparison with sota">
        </div>
    </section>
    <section>
        <h2> Example of GPT4SGG </h2>
        <div class="flex-container">
           <img src="resources/GPT4SGG-example.png" style="width:100%; height:auto;" alt="gpt4sgg-example">
        </div>	
    </section>
    <section>
	<h2> Samples of COCO-SG@GPT </h2>
        <div class="flex-container">
           <img src="resources/GPT4SGG-samples.png" style="width:100%; height:auto;" alt="gpt4sgg-samples">
        </div>	
    </section>
    <section>
        <h2> BibTeX </h2>
	<p> Please cite <b><span class="italic-text" style="color: #000080;">GPT4SGG</span></b> in your publications if it helps your research: </p>
	<button onclick="copyToClipboard()">Copy to Clipboard</button>
	<pre><code id="codeBlock">
@misc{chen2023gpt4sgg,
      title={GPT4SGG: Synthesizing Scene Graphs from Holistic and Region-specific Narratives}, 
      author={Zuyao Chen and Jinlin Wu and Zhen Lei and Zhaoxiang Zhang and Changwen Chen},
      year={2023},
      eprint={2312.04314},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
  <!-- JavaScript -->
  <script>
    function copyToClipboard() {
      const codeBlock = document.getElementById('codeBlock');
      const range = document.createRange();
      range.selectNode(codeBlock);
      window.getSelection().addRange(range);
      try {
        document.execCommand('copy');
        console.log('Copied to clipboard');
      } catch (err) {
        console.error('Failed to copy: ', err);
      }
      window.getSelection().removeAllRanges();
    }
  </script>
    </section>
     <div class="flex-container" style="text-align:center;margin-left:200px; margin-right:200px;width:35%;height:auto;">
    	<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=5T7TMJjrPpjnLv9T9NCEXRTSCqmcMrY-9ko6oydEsP0&cl=ffffff&w=a"></script>
     </div>
    <footer>
        <p>Acknowledgement: ChatGPT for website building</p>
    </footer>
</body>
</html>

